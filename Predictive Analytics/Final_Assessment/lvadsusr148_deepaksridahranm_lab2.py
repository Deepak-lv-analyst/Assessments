# -*- coding: utf-8 -*-
"""LVADSUSR148_DeepakSridahranM_Lab2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Azq1SXjCuCrPxwriMd8IUsOeK-nPOChy
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,classification_report

df = pd.read_csv('/content/penguins_classification.csv')
df.head(30)

df.shape

df.nunique()

df.dtypes

df.isnull().sum()

# df.isnull().sum()
# # kNNImputer to fill the missing values
# impute=KNNImputer()
# for i in df.select_dtypes(include='number').columns:
#   df[i]=impute.fit_transform(df[[i]])
# from sklearn.impute import KNNImputer

df=df.dropna()

df.isnull().sum()

df.duplicated().sum()

#treating outliers
numerical_columns = df.columns[df.dtypes != "object"]
numerical_columns
for i in numerical_columns:
  plt.figure(figsize = (10,6))
  sns.boxplot(data=df[i])
  plt.title(i)
  plt.ylabel('Values')
  plt.xticks(rotation = 45)
  plt.show()

def detect_and_treat_outliers(df,columns):

  for col in columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    median = df[col].median()
    df[col] = np.where((df[col] < lower_bound) | (df[col] > upper_bound), median, df[col])

  return df

df = detect_and_treat_outliers(df,numerical_columns)
numerical_columns = df.columns[df.dtypes != "object"]
numerical_data=df.select_dtypes(include=['float64','int64'])

# after removing outliers
for i in numerical_columns:
  plt.figure(figsize = (10,6))
  sns.boxplot(data=df[i])
  plt.title(i)
  plt.ylabel('Values')
  plt.xticks(rotation = 45)
  plt.show()

# #encoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

df["species"] = le.fit_transform(df["species"])
df["island"] = le.fit_transform(df["island"])

df.head()

plt.figure(figsize=(10,8))
sns.heatmap(numerical_data.corr(),annot=True,cmap="Blues")
plt.show()

plt.figure(figsize=(8,6))
sns.countplot(df,x="bill_length_mm")
plt.show()

plt.figure(figsize=(8,6))
sns.countplot(df,x="bill_depth_mm")
plt.show()

plt.figure(figsize=(8,6))
sns.countplot(df,x="flipper_length_mm")
plt.show()

plt.figure(figsize=(8,6))
sns.countplot(df,x="body_mass_g")
plt.show()

plt.figure(figsize=(8,6))
sns.countplot(df,x="year")
plt.show()

plt.figure(figsize=(10,6))
sns.scatterplot(x='flipper_length_mm',y='body_mass_g',data=df)
plt.show()

x = df.drop(["species"],axis=1)
y = df["species"]

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

#Logistic Regression
from sklearn.linear_model import LogisticRegression
reg = LogisticRegression()
reg.fit(x_train,y_train)
regpred = reg.predict(x_test)

from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
print(classification_report(y_test,regpred))
print("Accuracy of Logistic Regression is : ",accuracy_score(y_test,regpred)*100)

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

print("Mean Absolute Error : ",mean_absolute_error(y_test,regpred))
print("Mean Squared Error : ",mean_squared_error(y_test,regpred))
print("Root Mean Squared Error : ",np.sqrt(mean_squared_error(y_test,regpred)))
print("R2 Score : ",r2_score(y_test,regpred))

#Decision Tree classifier
from sklearn import tree
dtree = tree.DecisionTreeClassifier()
dtree.fit(x_train,y_train)
dtreepred = dtree.predict(x_test)

print(classification_report(y_test,dtreepred))
print("Accuracy of Decision Tree is : ",accuracy_score(y_test,dtreepred)*100)

print("Mean Absolute Error : ",mean_absolute_error(y_test,dtreepred))
print("Mean Squared Error : ",mean_squared_error(y_test,dtreepred))
print("Root Mean Squared Error : ",np.sqrt(mean_squared_error(y_test,dtreepred)))
print("R2 Score : ",r2_score(y_test,dtreepred))

#random forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100)
rf.fit(x_train,y_train)
rfpred = rf.predict(x_test)

print(classification_report(y_test,rfpred))
print("Accuracy of Random Forest is is : ",accuracy_score(y_test,rfpred)*100)

print("Mean Absolute Error : ",mean_absolute_error(y_test,rfpred))
print("Mean Squared Error : ",mean_squared_error(y_test,rfpred))
print("Root Mean Squared Error : ",np.sqrt(mean_squared_error(y_test,rfpred)))
print("R2 Score : ",r2_score(y_test,rfpred))

LR = accuracy_score(y_test,regpred)*100
DT = accuracy_score(y_test,dtreepred)*100
RF = accuracy_score(y_test,rfpred)*100

Model = ['LR','DT','RF']
Score = [LR, DT, RF]
barplot = plt.bar(x=Model,height=Score)
plt.show()

importances = rf.feature_importances_
feature_importances = pd.DataFrame({'feature': x.columns, 'importance': importances})

feature_importances = feature_importances.sort_values('importance', ascending=False)
print(feature_importances)