# -*- coding: utf-8 -*-
"""LVADSUSR148_DeepakSridharanM_Lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WyVtldm09ea4K9c1boTRHGRligAJF4B4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,classification_report

df = pd.read_csv('/content/Fare prediction.csv')
df.head(30)

df.shape

df.nunique()

df.dtypes

df.isnull().sum()

# df.isnull().sum()
# # kNNImputer to fill the missing values
# impute=KNNImputer()
# for i in df.select_dtypes(include='number').columns:
#   df[i]=impute.fit_transform(df[[i]])
# from sklearn.impute import KNNImputer

df=df.dropna()

df.isnull().sum()

df.duplicated().sum()

#treating outliers
numerical_columns = df.columns[df.dtypes != "object"]
numerical_columns
for i in numerical_columns:
  plt.figure(figsize = (10,6))
  sns.boxplot(data=df[i])
  plt.title(i)
  plt.ylabel('Values')
  plt.xticks(rotation = 45)
  plt.show()

def detect_and_treat_outliers(df,columns):

  for col in columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    median = df[col].median()
    df[col] = np.where((df[col] < lower_bound) | (df[col] > upper_bound), median, df[col])

  return df

df = detect_and_treat_outliers(df,numerical_columns)
numerical_columns = df.columns[df.dtypes != "object"]
numerical_data=df.select_dtypes(include=['float64','int64'])

# after removing outliers
for i in numerical_columns:
  plt.figure(figsize = (10,6))
  sns.boxplot(data=df[i])
  plt.title(i)
  plt.ylabel('Values')
  plt.xticks(rotation = 45)
  plt.show()

# #encoding
# from sklearn.preprocessing import LabelEncoder
# le = LabelEncoder()

# df[""] = le.fit_transform(df[" "])
# df[""] = le.fit_transform(df[""])
# df[""] = le.fit_transform(df[""])

plt.figure(figsize=(10,8))
sns.heatmap(numerical_data.corr(),annot=True,cmap="Blues")
plt.show()

plt.figure(figsize=(8,6))
sns.countplot(df,x="passenger_count")
plt.show()

plt.figure(figsize=(8,6))
sns.countplot(df,x="key")
plt.show()

plt.figure(figsize=(8,6))
sns.countplot(df,x="fare_amount")
plt.show()

plt.figure(figsize=(10,6))
sns.scatterplot(x='key',y='fare_amount',data=df)
plt.show()

x = df.drop(["fare_amount","key","pickup_datetime"],axis=1)
y = df["fare_amount"]

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

#Linear Regression
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(x_train,y_train)
regpred = reg.predict(x_test)

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

print("Mean Absolute Error : ",mean_absolute_error(y_test,regpred))
print("Mean Squared Error : ",mean_squared_error(y_test,regpred))
print("Root Mean Squared Error : ",np.sqrt(mean_squared_error(y_test,regpred)))
print("R2 Score : ",r2_score(y_test,regpred))

#Decision Tree regressor
from sklearn import tree
dtree = tree.DecisionTreeRegressor()
dtree.fit(x_train,y_train)
dtreepred = dtree.predict(x_test)

print("Mean Absolute Error : ",mean_absolute_error(y_test,dtreepred))
print("Mean Squared Error : ",mean_squared_error(y_test,dtreepred))
print("Root Mean Squared Error : ",np.sqrt(mean_squared_error(y_test,dtreepred)))
print("R2 Score : ",r2_score(y_test,dtreepred))

#random forest regressor
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100)
rf.fit(x_train,y_train)
rfpred = rf.predict(x_test)

print("Mean Absolute Error : ",mean_absolute_error(y_test,rfpred))
print("Mean Squared Error : ",mean_squared_error(y_test,rfpred))
print("Root Mean Squared Error : ",np.sqrt(mean_squared_error(y_test,rfpred)))
print("R2 Score : ",r2_score(y_test,rfpred))